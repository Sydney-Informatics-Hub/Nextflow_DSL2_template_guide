[
  {
    "objectID": "notebooks/modules.html",
    "href": "notebooks/modules.html",
    "title": "modules/ directory",
    "section": "",
    "text": "modules/ directory\n\n\n\n\n\n\nRelevant Nextflow components\n\n\n\n\nUse directives to control the execution of the current process.\nUse the input block to set the input channels of the current process.\nUse the output block to set the output channels of the current process.\nUse the script block to define the script that is executed by the process.\n\n\n\n\nWhat’s in modules/?\nThis directory contains all sub-workflows to be run with nextflow run main.nf. It is considered good practice to split out processes into separate .nf files and store them here, rather than including them all in the main.nf file. This directory is referenced in main.nf by include {x} from ./modules/process.\nEach module .nf script contains the process to be run, in addition to details of which container to be used, where to publish the output for the process.\n\n\nWhat is in each module file?\n\nDirectives\nNextflow process directives are optional settings that determine the execution of the current process. They can be provided at the top of the process body, before any other declaration blocks.\nSome examples you may wish to include are:\n\ndebug true to print stdout for each command being run\n\nmodule to specify environmental modules\ncontainer to specify a container to run the process\nerror strategy to define how a process error is managed\nlabels that can be applied to multiple processes\n\nIn the template we have provided an example of using a dynamic directive to modify the amount of computing resources requested by a process in case of a process failure and try to re-execute it using a higher limit:\nprocess process1 {\n    time { 2.hour * task.attempt }\n\n    errorStrategy { task.exitStatus in 137..143 ? 'retry' : 'terminate' }\n    maxRetries 2\n...\n}\nIn this example, if a task fails and returns an exit status between 137-143, it is resubmitted, otherwise it will terminate. The first time the process is run, task.attempt is set to 1 and it will request 2 hour of maximum execution time. If the first attempt of a task fails AND reports an exit status of 137-143, it will be resubmitted but this time task.attempt is 2, so the maximum execution time will be set to 4 hours. You can customise the number of attempts (maxRetries), the task exit status, and can specify time, memory, or cpus as resources.\n\n\nInput\nThe input block is required for each process, it defines the input channels for a process. A process must have at least one input and only one input block. You can specify values, paths, or files as inputs.\n\n\nOutput\nAn output block defines the output channels for a process. A process should have at least one output and only one output block. Nextflow DSL2 provides some flexibility regarding the creation of outputs. For example, we can use the emit option the assign a name identifier to a specific output. Assign a process’ output a specific name:\nprocess PROCESS1 {\n    output: \n        path 'sample*.bam', emit: sample_bam\n}\nThis name can be used within the main.nf workflow to reference the channel, whether that be as input for another process or to view the output:\nworkflow {\n    PROCESS1()\n    PROCESS1.out.samples_bam.view()\n}\nWe can also define an output as optional, meaning the process will not fail if an output is not generated by a task. Set an output as optional with the following:\noutput: \n    path(\"myFile.txt\"), optional: true\n\n\nScript\nThe script block defines the script executed by the process as a string expression. The script block must follow the input and output blocks at the bottom of a process scope.\n\nprocess process1 {\n\n    input: \n    path x \n\n    output:\n    path y \n\n    script:\n    \"\"\"\n    cat x \n    \"\"\"\n}",
    "crumbs": [
      "Home",
      "Template Components",
      "`modules/` directory"
    ]
  },
  {
    "objectID": "notebooks/bin.html",
    "href": "notebooks/bin.html",
    "title": "bin/ directory",
    "section": "",
    "text": "bin/ directory\nA strength of Nextflow is the fact it can be used to combine processes written in different languages. You may have custom scripts you would like to execute within your workflow and it is recommended these be stored in your /bin directory.\n\nWhat should go in bin/?\nIn the real world, workflows often use custom scripts written in languages like Bash, Perl, R, and Python. Simply place these scripts inside the bin/ directory in the workflow project root folder, as we have provided in this template. These scripts will automatically be added by Nextflow to your workflow execution $PATH.\n\n\n\n\n\n\nMake your scripts executable\n\n\n\nYour custom scripts must be executable for Nextflow to run them. You can give execution permissions for a custom script stored in bin/ with:\nchmod +x bin/customScript.sh",
    "crumbs": [
      "Home",
      "Template Components",
      "`bin/` directory"
    ]
  },
  {
    "objectID": "notebooks/components.html",
    "href": "notebooks/components.html",
    "title": "Template structure",
    "section": "",
    "text": "Template structure\nThe template employs a basic framework that allows users to customise and extend the workflow while adhering to code management best practices.\nThe workflow code-base is organised into a number of different executable files and directories. This organisaiton promotes code modularity, reusability, maintainability, and clarity. Each executable file included in the template contains comment lines, links to relevant Nextflow documentation, and implementation examples. To apply the template to your own workflows, you will need to replace examples with your own code.\nModularising the code base and splitting workflow processes and configuration files offers a few benefits:\n\nCode organisation and maintainability\nReadability and clarity\nCollaboration\nTesting and debugging\nCode reuse and extensibility\n\nNextflow is highly flexible, allowing users to implement workflows in a number of different ways. In this template, we provide one common implementation for structuring your Nextflow code base in a way that is easy to maintain and read.\nThe template’s code repository is organised into a number of files and directories:\nmyWorkflow-nf\n├── .github/ISSUE_TEMPLATE\n├── assets\n├── bin\n├── config\n├── modules\n├── main.nf\n├── LICENSE\n├── README.md\n└── nextflow.config\nConsider a basic Nextflow run command below, where a user will need to specify parameters and (optionally) a configuration profile:\n\n\nThe main.nf file is the executable file that identifies the workflow structure, inputs, and processes to pull form the modules/ directory.\nThe --parameter flag matches a parameter initialised in the nextflow.config file and applies it to the workflow execution.\nThe -profile flag is used to specify environment-specific configuration details such as a software implementation method and/or resource management.\n\nRunning a workflow that follows this template will output:\n\nA customisable execution message invoked when the workflow is run\nA customisable help message invoked using --help or failing to supply a required parameter\nA customisable completion message invoked when the workflow has finished running\nFiles generated by your processes, optionally these can be saved to a results directory\nWorkflow execution resource usage, trace, and timeline reports",
    "crumbs": [
      "Home",
      "Template Components"
    ]
  },
  {
    "objectID": "notebooks/software.html",
    "href": "notebooks/software.html",
    "title": "Software management",
    "section": "",
    "text": "Software management\nWORK IN PROGRESS",
    "crumbs": [
      "Home",
      "Best Practices",
      "**Software management**"
    ]
  },
  {
    "objectID": "notebooks/how-to.html",
    "href": "notebooks/how-to.html",
    "title": "How to guide",
    "section": "",
    "text": "Requirements:\n\nA GitHub account\nInstalled on your chosen development environment:\n\nNextflow (&gt;v20.10.0)\nGit\n\n\n\n\n\nOpen the DSL2 template repository on GitHub\nSelect the Use this template box and create a new repository\nName your repository, ending in -nf (this is required by cookiecutter gh action)\nSelect the Create repository from template box\n\nOnce you create a new repository using this template, a GitHub Action workflow will automatically be deployed. This workflow will populate your new repository with the skeleton template directory.\nWe recommend working in a development environment like VS Code to work with this template. If you’re working with VS Code, use the nf-core-extensionpack for some nice features like syntax highlighting. You can clone your copy of the template to your environment using git.\nFor example:\ngit clone https://github.com/georgiesamaha/myWorkflow-nf.git\n\n\n\nDepending on the system you’re working on there are a few options for installing and running Nextflow including reproducible options like bioconda and Singularity. See here for installation instructions.\nOnce you have installed Nextflow, you can configure it to run on your system. See here for some set up tips.",
    "crumbs": [
      "Home",
      "How-to Guides"
    ]
  },
  {
    "objectID": "notebooks/how-to.html#set-up",
    "href": "notebooks/how-to.html#set-up",
    "title": "How to guide",
    "section": "",
    "text": "Requirements:\n\nA GitHub account\nInstalled on your chosen development environment:\n\nNextflow (&gt;v20.10.0)\nGit\n\n\n\n\n\nOpen the DSL2 template repository on GitHub\nSelect the Use this template box and create a new repository\nName your repository, ending in -nf (this is required by cookiecutter gh action)\nSelect the Create repository from template box\n\nOnce you create a new repository using this template, a GitHub Action workflow will automatically be deployed. This workflow will populate your new repository with the skeleton template directory.\nWe recommend working in a development environment like VS Code to work with this template. If you’re working with VS Code, use the nf-core-extensionpack for some nice features like syntax highlighting. You can clone your copy of the template to your environment using git.\nFor example:\ngit clone https://github.com/georgiesamaha/myWorkflow-nf.git\n\n\n\nDepending on the system you’re working on there are a few options for installing and running Nextflow including reproducible options like bioconda and Singularity. See here for installation instructions.\nOnce you have installed Nextflow, you can configure it to run on your system. See here for some set up tips.",
    "crumbs": [
      "Home",
      "How-to Guides"
    ]
  },
  {
    "objectID": "notebooks/how-to.html#use-the-template",
    "href": "notebooks/how-to.html#use-the-template",
    "title": "How to guide",
    "section": "Use the template",
    "text": "Use the template\nThis template currently contains 3 processes as as part of the demo workflow. To use this workflow yourself, you will need to delete any processes associated with the demo. Each line of code inside the nextflow.config and main.nf associated with the demo workflow is prefixed by a comment line // DEMO CODE: DELETE FOR YOUR OWN WORKFLOWS.\nDelete the 3 demo process files (modules/check_input.nf, modules/group_samples.nf, modules/generate_report.nf) and use the template_process.nf to structure your own processes.\nSee the template_components sections in this user guide for what how to use all directories and files provided in this template.",
    "crumbs": [
      "Home",
      "How-to Guides"
    ]
  },
  {
    "objectID": "notebooks/bits.html",
    "href": "notebooks/bits.html",
    "title": "Extra bits",
    "section": "",
    "text": "There are multiple files and directories in this template. When downloaded, the template’s codebase is organised into the following directories and files. As you go through the template, you’ll notice a number of README.md files that explain what should be placed in the directory. Additionally, note:\n\n.github/ISSUE_TEMPLATE\nassets/\nLICENSE\nREADME.md\n\n\n\n\n\n\n\nNote\n\n\n\nWe highly recommend Nextflow’s Advanced Training materials to support your learning and implementing this template.\n\n\n\n\nGitHub issues are items that you and visitors to your code repository can use to plan, track, and discuss your code. By default these are all blank, however you can use templates to organise issues into different categories. For our purposes, we have organised issues into:\n\nBug report: to track and resolve problems in the code that cause it to not run as expected.\nFeature request: to suggest new ideas or features to be added to the workflow.\nQuestion: for general queries.\nBlank issue: for everything else.\n\nIn .github/ISSUE_TEMPLATE you’ll find four markdown files (.md), one each for the bug report template and feature request template. You’ll also find a config.yml which can be customised for your purposes.\nThese files are rendered in your GitHub repository website. For example, in one of our workflows, we’ve used only the bug report and feature request templates:\n\n\n\n\n\n\n\nImportant\n\n\n\nThe .github/ISSUE_TEMPLATE directory and its contents can be removed without impacting the functionality of your workflow.\n\n\nFor details on how to customise these issue templates, see GitHub’s documentation.\n\n\n\nYou can store auxillary files that your workflow may use in here. We don’t recommend storing data in your git repository but if you had a samplesheet you’d like to use for automated testing then it could be placed here. Additionally, you may wish to store the following:\n\nMultiqc config yaml files\nLogo image files\nEmail templates (e.g. nf-core/sarek email .txt and .html)\nParameter json schema\n\nFor example, in one of our workflows, we’ve included a MultiQC custom configuration file in the assets/ directory. We’ve then referenced this configuration file inside the MultiQC module file and provided a direct path to its location as a variable:\nprocess multiqc {\n    tag \"GENERATING REPORT: ${params.cohort_name}\" \n    publishDir \"${params.outdir}/multiqc\", mode: 'symlink'\n    container 'quay.io/biocontainers/multiqc:1.21--pyhdfd78af_0'\n\n    input:\n    path (multiqc_in)\n    path(params.multiqc_config)\n    \n    output:\n    path(\"*.html\")\n    path(\"*_data\")\n    path(\"*_plots\")\n\n    script: \n    def args = task.ext.args ?: ''\n    def multiqc_config = \"${baseDir}/assets/multiqc_config.yml\"\n    \"\"\"\n    multiqc . \\\n        --filename ${params.cohort_name}_multiqc \\\n        -c ${multiqc_config} \\\n        $args\n    \"\"\"\n}\n\n\n\nThe LICENSE file provides legal information about how your workflow code can be used, shared, and modified by others. This is important because it sets the terms and conditions under which your workflow or code can be distributed. In this template, we have included an open-source license, GNU GENERAL PUBLIC LICENSE, which is a popular choice for scientific software as it allows users to freely use, modify, and distribute the code.\nEnsure you choose a license that fits your project’s goals. See here for a registry of open source license options.\n\n\n\nThe README.md file is the primary documentation for your project and should provide an introduction, instructions on how to use the workflow, and additional context that potential users or contributors will find helpful. By maintaining a well-documented README.md, you’ll make it easier for others to understand and use your workflow.\nHere, we have used Australian BioCommons’ workflow documentation template.",
    "crumbs": [
      "Home",
      "Template Components",
      "Extra bits"
    ]
  },
  {
    "objectID": "notebooks/bits.html#githubissue_template",
    "href": "notebooks/bits.html#githubissue_template",
    "title": "Extra bits",
    "section": "",
    "text": "GitHub issues are items that you and visitors to your code repository can use to plan, track, and discuss your code. By default these are all blank, however you can use templates to organise issues into different categories. For our purposes, we have organised issues into:\n\nBug report: to track and resolve problems in the code that cause it to not run as expected.\nFeature request: to suggest new ideas or features to be added to the workflow.\nQuestion: for general queries.\nBlank issue: for everything else.\n\nIn .github/ISSUE_TEMPLATE you’ll find four markdown files (.md), one each for the bug report template and feature request template. You’ll also find a config.yml which can be customised for your purposes.\nThese files are rendered in your GitHub repository website. For example, in one of our workflows, we’ve used only the bug report and feature request templates:\n\n\n\n\n\n\n\nImportant\n\n\n\nThe .github/ISSUE_TEMPLATE directory and its contents can be removed without impacting the functionality of your workflow.\n\n\nFor details on how to customise these issue templates, see GitHub’s documentation.",
    "crumbs": [
      "Home",
      "Template Components",
      "Extra bits"
    ]
  },
  {
    "objectID": "notebooks/bits.html#assets",
    "href": "notebooks/bits.html#assets",
    "title": "Extra bits",
    "section": "",
    "text": "You can store auxillary files that your workflow may use in here. We don’t recommend storing data in your git repository but if you had a samplesheet you’d like to use for automated testing then it could be placed here. Additionally, you may wish to store the following:\n\nMultiqc config yaml files\nLogo image files\nEmail templates (e.g. nf-core/sarek email .txt and .html)\nParameter json schema\n\nFor example, in one of our workflows, we’ve included a MultiQC custom configuration file in the assets/ directory. We’ve then referenced this configuration file inside the MultiQC module file and provided a direct path to its location as a variable:\nprocess multiqc {\n    tag \"GENERATING REPORT: ${params.cohort_name}\" \n    publishDir \"${params.outdir}/multiqc\", mode: 'symlink'\n    container 'quay.io/biocontainers/multiqc:1.21--pyhdfd78af_0'\n\n    input:\n    path (multiqc_in)\n    path(params.multiqc_config)\n    \n    output:\n    path(\"*.html\")\n    path(\"*_data\")\n    path(\"*_plots\")\n\n    script: \n    def args = task.ext.args ?: ''\n    def multiqc_config = \"${baseDir}/assets/multiqc_config.yml\"\n    \"\"\"\n    multiqc . \\\n        --filename ${params.cohort_name}_multiqc \\\n        -c ${multiqc_config} \\\n        $args\n    \"\"\"\n}",
    "crumbs": [
      "Home",
      "Template Components",
      "Extra bits"
    ]
  },
  {
    "objectID": "notebooks/bits.html#license",
    "href": "notebooks/bits.html#license",
    "title": "Extra bits",
    "section": "",
    "text": "The LICENSE file provides legal information about how your workflow code can be used, shared, and modified by others. This is important because it sets the terms and conditions under which your workflow or code can be distributed. In this template, we have included an open-source license, GNU GENERAL PUBLIC LICENSE, which is a popular choice for scientific software as it allows users to freely use, modify, and distribute the code.\nEnsure you choose a license that fits your project’s goals. See here for a registry of open source license options.",
    "crumbs": [
      "Home",
      "Template Components",
      "Extra bits"
    ]
  },
  {
    "objectID": "notebooks/bits.html#readme.md",
    "href": "notebooks/bits.html#readme.md",
    "title": "Extra bits",
    "section": "",
    "text": "The README.md file is the primary documentation for your project and should provide an introduction, instructions on how to use the workflow, and additional context that potential users or contributors will find helpful. By maintaining a well-documented README.md, you’ll make it easier for others to understand and use your workflow.\nHere, we have used Australian BioCommons’ workflow documentation template.",
    "crumbs": [
      "Home",
      "Template Components",
      "Extra bits"
    ]
  },
  {
    "objectID": "notebooks/configs.html",
    "href": "notebooks/configs.html",
    "title": "config/ directory",
    "section": "",
    "text": "config/ directory\n\n\n\n\n\n\nRelevant Nextflow components\n\n\n\n\nUse\n\n\n\n\nWhat’s in config/?\nThis directory contains various profile modules for configuring the pipeline run. Some care should be taken when using these config profiles. See the Nextflow documentation for more details.\nThis directory contains the following profiles:\n\nnimbus: this profile is specific to Pawsey Supercomputing Centre’s Nimbus cloud. It enables the use of Docker.\nstandard: this is the default profile.\nsetonix: this profile is specific to Pawsey Supercomputing Centre’s Setonix HPC. It enables the use of the SLURM job scheduler and Singularity.\n\ngadi: this profile is specific to the National Computational Infrastructure’s Gadi HPC. It enables the use of the PBS Pro job scheduler and Singularity\n\n\n\nHow do I write a custom config file?\nWhile we have provided a number of recommended configuration features inside our template’s nextflow.config file, these may not port well between different infrastructures. For that reason we have provided some additional config examples for national HPC and cloud platforms. There are many different ways to configure a Nextflow workflow and your need to configure a workflow will depend on the needs of your workflow users, such as:\n\nIncrease the resources to take advantage of high CPU or high memory infrastructures\nRun on a HPC or cloud infrastructure\nExecute specific modules on specific node types on a cluster\nUse a different software execution method\nSelectively adjust the execution of one or a few processes\n\nWe recommend you take a look at the examples provided in the config/ directory of this template, the Nextflow documentation regarding configuration and our customising nf-core workshop materials to get some ideas on how to write custom Nextflow configuration files.",
    "crumbs": [
      "Home",
      "Template Components",
      "`config/` directory"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "We developed a template-nf to aid beginners in developing their own Nextflow workflows. Here, we guide you through how to use our workflow template to develop your own Nextflow pipelines.\nNextflow is open source and scalable workflow management software for bioinformatics. It enables the development and running of integrated, reproducible workflows consisting of multiple processes, various environment management systems, scripting languages, and software packages.\nWhile Nextflow is designed to have a minimal learning curve, its extensive capabilities, use of Groovy syntax, and comprehensive documentation can be overwhelming for users who aren’t well versed in programming and software development.\nThis user guide is under active development and will be updated progressively.  \n\nWho is template-nf for?\nOur Nextflow template is suitable for:\n\nNextflow newcomers looking for a low barrier to entry, structured starting point, and guidance.\nCustom workflow developers looking for a simple scaffold which can be extended and modified as needed.\nCollaborative teams looking for a standard and consistent workflow code base structure.\nScalable workflow developers looking for a scalable and reproducible solution for their data analysis and processing needs.\n\nIt is not suitable for:\n\nThose wishing to create and contribute to public nf-core workflows, as it is not nf-core compatible.\nThose creating simple or single-task workflows with only a few tasks and minimal complexity.\nThose needing to rapidly prototype a workflow or perform exploratory analysis.\nThose with no previous command-line and bash experience.\n\n\n\nGetting started\nGet a feel for the template and how to use it with the in-built demo workflow and our tutorial.\n\n\n\n\n\n\nAttention newcomers!\n\n\n\nIf you are new to this template and/or Nextflow, we recommend you do the following before applying the template to your own workflow:\n\nFamiliarise yourself with Nextflow\nClone the template repository\nExplore the template structure\n\n\n\n\n\nSite contents\n\nTutorial\nHow-to guide\nTemplate components\nBest practices\nSupporting resources  \n\n\n\nAcknowledgements\nTemplate-nf and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney in partnership with the Australian BioCommons (NCRIS via Bioplatforms Australia)."
  },
  {
    "objectID": "notebooks/demo.html",
    "href": "notebooks/demo.html",
    "title": "Template demo",
    "section": "",
    "text": "template-nf contains a demonstration workflow that can help you explore the template’s functionality. This demo accepts a samplesheet as input and runs 3 processes, demonstrating various aspects of Nextflow functionality.\nYou will need to have Nextflow installed on your system to run the demo workflow.\nWe’ve designed this demo to show off the most simple aspects of the template, we have not included the use of the config/ directory as configuration profiles are often infrastructure dependent.\nThe demo workflow consists of 3 processes and requires only one input parameter be defined. It accepts samplesheet that can be found in the assets/ directory.\n\nWe have structured this demo to show how one can use inbuilt Nextflow features to do common tasks like run custom scripts, pass outputs of one process as inputs to another, publish outputs to a results directory, and process samples specified in a samplesheet in parallel.\n\n\nImagine the processes run by this demo are the beginning of a very complex workflow where a user needs to batch samples for further processing depending on their sequencing platform. In this scenario, we’ve got a single samplesheet in csv format with the following structure (in your code see assets/samplesheet.csv):\n \n\n\n\n\n\n\n\n\n\n\n\n\nsample\nfq1\nfq2\nplatform\nlibrary\ncenter\n\n\n\n\nearlycasualcaiman\nearlycasualcaiman_XXXXXX_1_1k.fastq.gz\nearlycasualcaiman_XXXXXX_2_1k.fastq.gz\nillumina\n1\nnfcore\n\n\nhugelymodelbat\nhugelymodelbat_XXXXXX_1_1k.fastq.gz\nhugelymodelbat_XXXXXX_2_1k.fastq.gz\npacbio\n1\nnfcore\n\n\nvapidkangaroo\nvapidkangaroo_XXXXXX_1_1k.fastq.gz\nvapidkangaroo_XXXXXX_2_1k.fastq.gz\npacbio\n1\nnfcore\n\n\n\n\n \nWe need to organise samples based on their sequencing platform for subsequent processing as pacbio and illumina reads require different software and processing methods.\nUpon executing this workflow, the samplesheet.csv file will be passed to the first process check_input, which runs a custom script stored in the bin/ directory to validate the structure of the samplesheet as per the conditions set out in the custom script. So these processes are going to validate our original input samplesheet format, group and split the samples into 2 new samplesheets, and summarise the batches separately.\n\nOnce successfully validated, the validated samplesheet is then passed to the next process group_samples to create 2 separate samplesheets based on which sequencing platform the samples specified in each row have been processed on.\nAfter this, each platform-specific samplesheet will be summarised by the generate_report process in parallel, creating a separate summary text file for each.\n\n\n\nEach process file follows the structure laid out in modules/template_process.nf. We’ve used:\n\nComment lines // to explain sections of code\ntag to associate each process execution with a custom label.\npublishDir to save outputs of selected processes to --outdir.\ninput: to define the input channels of a process, similar to function arguments.\noutput: to define output channels of a process, similar to function outputs.\nemit: to define the name of the output channel, which can be used to access the channel by name from the process output\nscript: to define, as a string expression, the script or code that is executed by a process.\n\n// Define the process\nprocess generate_report {\n    // Define directives \n    // See: https://nextflow.io/docs/edge/process.html#processes\n    debug = false //turn to true to print command stdout to screen\n    tag \"\" \n    publishDir \"${params.outdir}/\", mode: 'copy'\n  container '' \n\n    // Define input \n    // See: https://www.nextflow.io/docs/latest/process.html#inputs\n    input:\n    path(\"\")\n\n    // Define output(s)\n    // See: https://www.nextflow.io/docs/latest/process.html#outputs\n    output:\n    path(\"\")\n\n    // Define code to execute \n    // See: https://www.nextflow.io/docs/latest/process.html#script\n    script:\n    \"\"\"\n\n    \"\"\"\n}\n\n\n\n\n\n\n\n\ngit clone https://github.com/yourusername/your-pipeline.git\n\n\n\ncd your-pipeline\nNote the different directories and files in the repository. We’ve provided one common implementation for structuring your Nextflow code base in a way that is easy to maintain and read. It consists of\n\nThe template’s code repository is organised into a number of files and directories. Hidden directories prefixed with a . can be ignored for now, they are useful for configuring git and github and aren’t related to running your workflow. The code used in the demo workflow are:\n\nmain.nf: the primary execution script, it contains workflow structure, processes, and channels.\nnextflow.config: the configuration file, it contains a number of property definitions that are used by the pipeline.\nassets/: stores auxillary files. We’ve stored our example samplesheet.csv here.\nbin/: stores custom scripts to be executed by Nextflow processes. We’ve stored a custom script samplesheetchecker.sh run by the first process of this workflow here.\nmodules/: contains code run by each process executed by the workflow. Processes have been separated into different .nf files for the sake of readability and easy maintenance.\n\n\n\n\nnextflow run main.nf --input assets/samplesheet.csv\nTake a look at the stdout printed to the screen. The demo workflow configuration and parameter customisations are documented here. You can use this to confirm if your parameters have been correctly passed to the run command.\n\nWhen you execute the workflow on the command line, Nextflow will print a bunch of information to the screen. Some of this information is generated automatically (orange box), others are specified in the main.nf (green boxes). The information in the green boxes have been defined in our main.nf using log.info. It is an entirely optional feature and can be edited as you need.\n\n\n\n\n\n\nlog.info usage\n\n\n\nThis command can be used to print multiline information using Groovy’s logger functionality. Instead of writing a series of println commands, it can be used to include a multiline message.\nlog.info \"\"\"\\\n    This is a multiline message. \n    It can even capture parameters like\n    results : ${params.outdir}\n\"\"\"\n\n\nOpen the main.nf file to see how we’ve implemented this for summarising the workflow’s execution, pre-execution and post-execution:\n// Print a header for your pipeline \nlog.info \"\"\"\\\n\n=======================================================================================\nName of the pipeline - nf \n=======================================================================================\n\nCreated by &lt;YOUR NAME&gt; \nFind documentation @ https://sydney-informatics-hub.github.io/template-nf-guide/\nCite this pipeline @ INSERT DOI\n\n=======================================================================================\nWorkflow run parameters \n=======================================================================================\ninput       : ${params.input}\nresults     : ${params.outdir}\nworkDir     : ${workflow.workDir}\n=======================================================================================\n\n\"\"\"\n// Print workflow execution summary \nworkflow.onComplete {\nsummary = \"\"\"\n=======================================================================================\nWorkflow execution summary\n=======================================================================================\n\nDuration    : ${workflow.duration}\nSuccess     : ${workflow.success}\nworkDir     : ${workflow.workDir}\nExit status : ${workflow.exitStatus}\nresults     : ${params.outdir}\n\n=======================================================================================\n  \"\"\"\nprintln summary\n\n}\nAs the workflow starts, you will also see a number of processes spawn out underneath this. In Nextflow, processes are executed independently and can run in parallel. Nextflow manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied. In our demo, we’ve got one task per check_input and group_samples process, and two tasks as the generate_report process is run over each sequencer-specific samplesheet in parallel.\n\nBecause of how the data flows from process 1 (check_input) to process 2 (group_samples) to process 3 (generate_report) in our workflow, they will run in that order.\n\n\n\nIf you can see the workflow exercution summary has run without error, then the pipeline ran successfully. List (ls -la) the contents of your directory, you’ll see a few new directories (and a hidden directory and log file) have been created:\n\nThe work/ directory\nAs each task is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this task that we need to troubleshoot a failed task.\n\nNotice how the work directories correspond to the apparently random number/letter codes printed to the screen next to each process upon workflow execution. Each task in a Nextflow pipeline is assigned a unique identifier based on the input data, parameters, and code used. The output of the task is then saved (cached) in a uniquely named subdirectory within the work directory.\n\n\n\n\n\n\nProcess caching and the work directory\n\n\n\nEach process in Nextflow generates a unique hash key that represents the combination of the process script, input data, and any parameters used. This hash key ensures that any change in the input, script, or parameters results in a new hash, and thus a new subdirectory in the work directory.\nWhen a process is executed, Nextflow checks if there is already a subdirectory in the work directory that corresponds to the same hash key. If it finds one, this means that the process with the same inputs and parameters has been run before, and the cached results are available. If a matching directory is found, Nextflow reuses the cached output instead of rerunning the process. This is especially useful when working on large datasets or complex workflows where re-running every step can be time-consuming and computationally expensive.\nIf a workflow is interrupted or if you decide to make changes and rerun it, Nextflow can resume from where it left off. Since the results of previously completed tasks are cached in the work directory, Nextflow will skip these tasks and only run the processes that are new or whose inputs have changed.\n\n\nThe results/ directory\nAll final outputs for this workflow will be presented in a directory specified by the --outdir flag which is a custom parameter we have defined in the nextflow.config as params.ourdir. Note the default is set to results/.\n\nIf you were to rerun the workflow, using the --outdir flag and a different value, you could change the name of the output directory:\nnextflow run main.nf --input assets/samplesheet.csv --outdir different_out\n\n\n\n\n\n\nRendering workflow outputs\n\n\n\nAll Nextflow task outputs are saved to their unique subdirectories within work/ automatically. Although the work directory is crucial during the workflow execution, it’s typically not the place where you store final results. It’s more of a temporary workspace that can be cleaned up after the workflow is completed.\nNextflow requires users to explicitly instruct it to save final outputs after the workflow completes. Unlike the work directory, the location of this is specified by the user. Setting up an output directory is something we must do intentionally at the process level.\n\n\nNote the contents of the results/ directory and how this has been defined by the demo workflow’s developer in the process scopes inside the modules/ directory using publishDir. For example, the group_samples process:\nprocess group_samples {\n    tag \"INPUT: ${checked_samplesheet.fileName}\"\n    publishDir \"${params.outdir}\", mode: 'copy'\n    \n    input:\n    path(checked_samplesheet)\n\n    output:\n    path(\"samplesheet_illumina.csv\"), emit: illumina\n    path(\"samplesheet_pacbio.csv\"), emit: pacbio\n\n    script:\n    \"\"\"\n    awk -F, '\\$4 == \"illumina\"' OFS=, ${checked_samplesheet} &gt; samplesheet_illumina.csv\n    awk -F, '\\$4 == \"pacbio\"' OFS=, ${checked_samplesheet} &gt; samplesheet_pacbio.csv\n    \"\"\"\n}\nHere, we’ve chosen to output the files created by this process as a copy of what has been saved to the work/ directory. Alternatively, to save space we could have used symlink to reduce the creation of additional files.\nThe .nextflow/ directory\nThis directory contains a cache subdirectory to store cached data such as downloaded files and can be used to speed up subsequent pipeline runs. It also contains a history file which contains a record of pipeline executions including run time, the unique run name, and command line arguments used.\nThe .nextflow.log file\nThis file is created by Nextflow during the execution of a pipeline and contains information about all processes and any warnings or errors that occurred during execution.",
    "crumbs": [
      "Home",
      "Tutorial"
    ]
  },
  {
    "objectID": "notebooks/demo.html#the-demo-scenario",
    "href": "notebooks/demo.html#the-demo-scenario",
    "title": "Template demo",
    "section": "",
    "text": "Imagine the processes run by this demo are the beginning of a very complex workflow where a user needs to batch samples for further processing depending on their sequencing platform. In this scenario, we’ve got a single samplesheet in csv format with the following structure (in your code see assets/samplesheet.csv):\n \n\n\n\n\n\n\n\n\n\n\n\n\nsample\nfq1\nfq2\nplatform\nlibrary\ncenter\n\n\n\n\nearlycasualcaiman\nearlycasualcaiman_XXXXXX_1_1k.fastq.gz\nearlycasualcaiman_XXXXXX_2_1k.fastq.gz\nillumina\n1\nnfcore\n\n\nhugelymodelbat\nhugelymodelbat_XXXXXX_1_1k.fastq.gz\nhugelymodelbat_XXXXXX_2_1k.fastq.gz\npacbio\n1\nnfcore\n\n\nvapidkangaroo\nvapidkangaroo_XXXXXX_1_1k.fastq.gz\nvapidkangaroo_XXXXXX_2_1k.fastq.gz\npacbio\n1\nnfcore\n\n\n\n\n \nWe need to organise samples based on their sequencing platform for subsequent processing as pacbio and illumina reads require different software and processing methods.\nUpon executing this workflow, the samplesheet.csv file will be passed to the first process check_input, which runs a custom script stored in the bin/ directory to validate the structure of the samplesheet as per the conditions set out in the custom script. So these processes are going to validate our original input samplesheet format, group and split the samples into 2 new samplesheets, and summarise the batches separately.\n\nOnce successfully validated, the validated samplesheet is then passed to the next process group_samples to create 2 separate samplesheets based on which sequencing platform the samples specified in each row have been processed on.\nAfter this, each platform-specific samplesheet will be summarised by the generate_report process in parallel, creating a separate summary text file for each.",
    "crumbs": [
      "Home",
      "Tutorial"
    ]
  },
  {
    "objectID": "notebooks/demo.html#the-processes",
    "href": "notebooks/demo.html#the-processes",
    "title": "Template demo",
    "section": "",
    "text": "Each process file follows the structure laid out in modules/template_process.nf. We’ve used:\n\nComment lines // to explain sections of code\ntag to associate each process execution with a custom label.\npublishDir to save outputs of selected processes to --outdir.\ninput: to define the input channels of a process, similar to function arguments.\noutput: to define output channels of a process, similar to function outputs.\nemit: to define the name of the output channel, which can be used to access the channel by name from the process output\nscript: to define, as a string expression, the script or code that is executed by a process.\n\n// Define the process\nprocess generate_report {\n    // Define directives \n    // See: https://nextflow.io/docs/edge/process.html#processes\n    debug = false //turn to true to print command stdout to screen\n    tag \"\" \n    publishDir \"${params.outdir}/\", mode: 'copy'\n  container '' \n\n    // Define input \n    // See: https://www.nextflow.io/docs/latest/process.html#inputs\n    input:\n    path(\"\")\n\n    // Define output(s)\n    // See: https://www.nextflow.io/docs/latest/process.html#outputs\n    output:\n    path(\"\")\n\n    // Define code to execute \n    // See: https://www.nextflow.io/docs/latest/process.html#script\n    script:\n    \"\"\"\n\n    \"\"\"\n}",
    "crumbs": [
      "Home",
      "Tutorial"
    ]
  },
  {
    "objectID": "notebooks/demo.html#run-the-demo",
    "href": "notebooks/demo.html#run-the-demo",
    "title": "Template demo",
    "section": "",
    "text": "git clone https://github.com/yourusername/your-pipeline.git\n\n\n\ncd your-pipeline\nNote the different directories and files in the repository. We’ve provided one common implementation for structuring your Nextflow code base in a way that is easy to maintain and read. It consists of\n\nThe template’s code repository is organised into a number of files and directories. Hidden directories prefixed with a . can be ignored for now, they are useful for configuring git and github and aren’t related to running your workflow. The code used in the demo workflow are:\n\nmain.nf: the primary execution script, it contains workflow structure, processes, and channels.\nnextflow.config: the configuration file, it contains a number of property definitions that are used by the pipeline.\nassets/: stores auxillary files. We’ve stored our example samplesheet.csv here.\nbin/: stores custom scripts to be executed by Nextflow processes. We’ve stored a custom script samplesheetchecker.sh run by the first process of this workflow here.\nmodules/: contains code run by each process executed by the workflow. Processes have been separated into different .nf files for the sake of readability and easy maintenance.\n\n\n\n\nnextflow run main.nf --input assets/samplesheet.csv\nTake a look at the stdout printed to the screen. The demo workflow configuration and parameter customisations are documented here. You can use this to confirm if your parameters have been correctly passed to the run command.\n\nWhen you execute the workflow on the command line, Nextflow will print a bunch of information to the screen. Some of this information is generated automatically (orange box), others are specified in the main.nf (green boxes). The information in the green boxes have been defined in our main.nf using log.info. It is an entirely optional feature and can be edited as you need.\n\n\n\n\n\n\nlog.info usage\n\n\n\nThis command can be used to print multiline information using Groovy’s logger functionality. Instead of writing a series of println commands, it can be used to include a multiline message.\nlog.info \"\"\"\\\n    This is a multiline message. \n    It can even capture parameters like\n    results : ${params.outdir}\n\"\"\"\n\n\nOpen the main.nf file to see how we’ve implemented this for summarising the workflow’s execution, pre-execution and post-execution:\n// Print a header for your pipeline \nlog.info \"\"\"\\\n\n=======================================================================================\nName of the pipeline - nf \n=======================================================================================\n\nCreated by &lt;YOUR NAME&gt; \nFind documentation @ https://sydney-informatics-hub.github.io/template-nf-guide/\nCite this pipeline @ INSERT DOI\n\n=======================================================================================\nWorkflow run parameters \n=======================================================================================\ninput       : ${params.input}\nresults     : ${params.outdir}\nworkDir     : ${workflow.workDir}\n=======================================================================================\n\n\"\"\"\n// Print workflow execution summary \nworkflow.onComplete {\nsummary = \"\"\"\n=======================================================================================\nWorkflow execution summary\n=======================================================================================\n\nDuration    : ${workflow.duration}\nSuccess     : ${workflow.success}\nworkDir     : ${workflow.workDir}\nExit status : ${workflow.exitStatus}\nresults     : ${params.outdir}\n\n=======================================================================================\n  \"\"\"\nprintln summary\n\n}\nAs the workflow starts, you will also see a number of processes spawn out underneath this. In Nextflow, processes are executed independently and can run in parallel. Nextflow manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied. In our demo, we’ve got one task per check_input and group_samples process, and two tasks as the generate_report process is run over each sequencer-specific samplesheet in parallel.\n\nBecause of how the data flows from process 1 (check_input) to process 2 (group_samples) to process 3 (generate_report) in our workflow, they will run in that order.\n\n\n\nIf you can see the workflow exercution summary has run without error, then the pipeline ran successfully. List (ls -la) the contents of your directory, you’ll see a few new directories (and a hidden directory and log file) have been created:\n\nThe work/ directory\nAs each task is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this task that we need to troubleshoot a failed task.\n\nNotice how the work directories correspond to the apparently random number/letter codes printed to the screen next to each process upon workflow execution. Each task in a Nextflow pipeline is assigned a unique identifier based on the input data, parameters, and code used. The output of the task is then saved (cached) in a uniquely named subdirectory within the work directory.\n\n\n\n\n\n\nProcess caching and the work directory\n\n\n\nEach process in Nextflow generates a unique hash key that represents the combination of the process script, input data, and any parameters used. This hash key ensures that any change in the input, script, or parameters results in a new hash, and thus a new subdirectory in the work directory.\nWhen a process is executed, Nextflow checks if there is already a subdirectory in the work directory that corresponds to the same hash key. If it finds one, this means that the process with the same inputs and parameters has been run before, and the cached results are available. If a matching directory is found, Nextflow reuses the cached output instead of rerunning the process. This is especially useful when working on large datasets or complex workflows where re-running every step can be time-consuming and computationally expensive.\nIf a workflow is interrupted or if you decide to make changes and rerun it, Nextflow can resume from where it left off. Since the results of previously completed tasks are cached in the work directory, Nextflow will skip these tasks and only run the processes that are new or whose inputs have changed.\n\n\nThe results/ directory\nAll final outputs for this workflow will be presented in a directory specified by the --outdir flag which is a custom parameter we have defined in the nextflow.config as params.ourdir. Note the default is set to results/.\n\nIf you were to rerun the workflow, using the --outdir flag and a different value, you could change the name of the output directory:\nnextflow run main.nf --input assets/samplesheet.csv --outdir different_out\n\n\n\n\n\n\nRendering workflow outputs\n\n\n\nAll Nextflow task outputs are saved to their unique subdirectories within work/ automatically. Although the work directory is crucial during the workflow execution, it’s typically not the place where you store final results. It’s more of a temporary workspace that can be cleaned up after the workflow is completed.\nNextflow requires users to explicitly instruct it to save final outputs after the workflow completes. Unlike the work directory, the location of this is specified by the user. Setting up an output directory is something we must do intentionally at the process level.\n\n\nNote the contents of the results/ directory and how this has been defined by the demo workflow’s developer in the process scopes inside the modules/ directory using publishDir. For example, the group_samples process:\nprocess group_samples {\n    tag \"INPUT: ${checked_samplesheet.fileName}\"\n    publishDir \"${params.outdir}\", mode: 'copy'\n    \n    input:\n    path(checked_samplesheet)\n\n    output:\n    path(\"samplesheet_illumina.csv\"), emit: illumina\n    path(\"samplesheet_pacbio.csv\"), emit: pacbio\n\n    script:\n    \"\"\"\n    awk -F, '\\$4 == \"illumina\"' OFS=, ${checked_samplesheet} &gt; samplesheet_illumina.csv\n    awk -F, '\\$4 == \"pacbio\"' OFS=, ${checked_samplesheet} &gt; samplesheet_pacbio.csv\n    \"\"\"\n}\nHere, we’ve chosen to output the files created by this process as a copy of what has been saved to the work/ directory. Alternatively, to save space we could have used symlink to reduce the creation of additional files.\nThe .nextflow/ directory\nThis directory contains a cache subdirectory to store cached data such as downloaded files and can be used to speed up subsequent pipeline runs. It also contains a history file which contains a record of pipeline executions including run time, the unique run name, and command line arguments used.\nThe .nextflow.log file\nThis file is created by Nextflow during the execution of a pipeline and contains information about all processes and any warnings or errors that occurred during execution.",
    "crumbs": [
      "Home",
      "Tutorial"
    ]
  },
  {
    "objectID": "notebooks/main-nf.html",
    "href": "notebooks/main-nf.html",
    "title": "main.nf",
    "section": "",
    "text": "Relevant Nextflow components\n\n\n\n\nUse the keyword include to import modules into the workflow.\nUse the keyword workflow scope to compose a set of processes, channels, and operators.\nUse channels for data flow between processes\nUse operators to create and manipulate channels.\n\n\n\nThis is the primary script file or entry point for defining and executing a Nextflow workflow. The main.nf script contains the workflow’s structure, processes, and channels. This file doesn’t have to be called main.nf but this is common practice in Nextflow.\n\nWhat’s in main.nf?\nThis file contains the following essentials:\n\nProcess definitions\nChannel definitions\nWorkflow structure\n\n\nProcesses\nConsider processes as individual units of work or steps to be executed by a workflow. While you can opt to define all processes within the main.nf script (like here), we have chosen to focus on modularity with this template. As such, processes inside the template are saved as individual scripts inside the modules/ directory and imported into the main.nf script, using the include keyword:\ninclude { processName } from './modules/moduleName'\nEach process is also specified again inside the workflow{} scope along with its inputs. See the modules/ directory section for details on how process modules connect to their definitions inside the main.nf.\n\n\nChannels\nThere are lots of ways to define and apply channels in Nextflow. In Nextflow, channels are essential for connecting tasks to one another in the workflow definition. Channels are defined within the workflow scope in the workflow template. In the template, we’ve defined an example channel called input that uses the fromPath() factory to capture a parameterised input file so users can provide their own custom input files to the workflow.\nObserve how the input channel is consumed by process1 below:\nworkflow {\n    input = Channel.fromPath(\"${params.input}\")\n\n    process1(input)\n}\nSometimes you will need to apply methods called operators to transform data inside channels. For example, here we used a combination of basic operators to:\n\nSplit rows into different columns (.splitCsv)\nGroup those split fields for each row to pair a specific file to a specific sample (.map)\n\ninput = checkInputs.out\n    .splitCsv(header: true, sep:\"\\t\")\n    .map { row -&gt; tuple(row.sampleID, file(row.bam))}\n\n\nThe workflow\nThe workflow{} scope contains all components required to invoke one or more processes. In the template, we’ve used an if else loop to first invoke a help command to ensure the workflow is being run with the correct parameters and then run the processes imported using the include scope.\nInside the template, two example processes and their inputs/outputs are connected as:\nworkflow {\n\n    input = Channel.fromPath(\"${params.input}\")\n\n    // Run process 1 \n    processName1(input)\n    \n    // Run process 2 which takes output of process 1 \n    processName2(processOne.out.File)\n}\nWorkflows can be structured with more flexibility and complexity, see here for some examples.\n\n\nAdditional features\nSome additional features that we have provided in the main.nf file are intended to make your workflows easier to run and troubleshoot. They are not currently well documented in the Nextflow user guide. We have used Nextflow’s interpretation of the Groovy logger function to print messages to the screen to make the workflow more interactive and clarify execution. The log functions we have used include the following and can be removed without affecting the workflow execution.\nA customisable execution message\nThis will be printed to the screen when the workflow is run. Edit the text inside the header text block:\nlog.info \"\"\"\\\n    YOU CAN PUT ANYTHING YOU WANT IN HERE \n\"\"\".stripIndent()`\nSome suggestions for things to include here:\n\nThe name of your workflow\nA DOI for citation purposes\nPrint specified parameters\n\nA customisable help message\nThis will be printed to the screen when the --help parameter is used or a required parameter is not supplied. You can edit the message to be printed inside the help message text block:\ndef helpMessage() {\n    log.info\"\"\"\n        YOU CAN PUT ANYTHING YOU WANT IN HERE \n    \"\"\".stripIndent()\n}\nSome suggestions to include here:\n\nThe suggested run command\nRequired parameters and a short explainer\nOptional parameters and a short explainer\n\nA customisable completion message\nThis will be printed to the screen when the workflow has finished running. You can edit the message to be printed inside the summary message block:\nworkflow.onComplete {\nsummary = \"\"\"\n    YOU CAN PUT ANYTHING YOU WANT IN HERE \n  \"\"\"\nprintln summary\n}",
    "crumbs": [
      "Home",
      "Template Components",
      "`main.nf`"
    ]
  },
  {
    "objectID": "notebooks/guides.html",
    "href": "notebooks/guides.html",
    "title": "Guides",
    "section": "",
    "text": "Guides\nWORK IN PROGRESS"
  },
  {
    "objectID": "notebooks/resource-config.html",
    "href": "notebooks/resource-config.html",
    "title": "Resource configuration",
    "section": "",
    "text": "WORK IN PROGRESS\nResource configuration in Nextflow workflows can be challenging due to the diversity of computational environments we each work in. Each environment has unique resource constraints and management systems which can complicate the allocation of resources like CPUs, memory, and storage.\nEnsuring you’re resource efficient will minimise the runtime and reduce the cost of running your workflows. Poorly configured workflows can lead to failed jobs, wasted computational time, and overuse of resources, particularly in HPC and cloud environments.\nhttps://nextflow.io/blog/2024/optimizing-nextflow-for-hpc-and-cloud-at-scale.html",
    "crumbs": [
      "Home",
      "Best Practices",
      "**Resource configuration**"
    ]
  },
  {
    "objectID": "notebooks/resource-config.html#nextflow-configuration-files",
    "href": "notebooks/resource-config.html#nextflow-configuration-files",
    "title": "Resource configuration",
    "section": "Nextflow configuration files",
    "text": "Nextflow configuration files\nThe core of resource configuration in Nextflow should be contained within the nextflow.config and any other custom configuration files. When a workflow is executed with nextflow run main.nf, Nextflow looks for the nextflow.config and any other .config files in the current directory and the base directory of the execution script. It will also check $HOME/.nextflow/config. When more than 1 of these files exists, they are merged so that the default settings in the nextflow.config are overwritten as required.\nThese configuration files allow you to specify settings for resources such as:\n\ncpus\nmemory\ntime\nexecutor\nenv variables\n\nHere is a basic example of how you can set these resources for all processes in a workflow within the process scope in a configuration file:\nprocess {\n  cpus = 2\n  memory = '4.GB'\n  time = '2.h'\n}\nIn Nextflow, resource directives are specified within the process block. These directives control how many CPUs, how much memory, and how much time each process is allocated. While a default minimum resource allocation can be suitable in some workflows, this will not always work in more complex workflows and you will need to configure resources per process. Here is an example of how you’d configure the resources for a specific process, overwriting default settings for that process:\nprocess {\n  cpus = 2\n  memory = '4.GB'\n  time = '2.h'\n  \n  // Provide additional memory for indexing process\n  withName: 'STAR_INDEX' {\n    cpus = 1\n    memory = '32.GB'\n  }\n}",
    "crumbs": [
      "Home",
      "Best Practices",
      "**Resource configuration**"
    ]
  },
  {
    "objectID": "notebooks/resource-config.html#dynamic-resource-allocations",
    "href": "notebooks/resource-config.html#dynamic-resource-allocations",
    "title": "Resource configuration",
    "section": "Dynamic resource allocations",
    "text": "Dynamic resource allocations\nNextflow also allows you to dynamically allocate resources based on the input data size or task type. For example, you might need to adjust memory based on the size of an input file:",
    "crumbs": [
      "Home",
      "Best Practices",
      "**Resource configuration**"
    ]
  },
  {
    "objectID": "notebooks/nextflow-config.html",
    "href": "notebooks/nextflow-config.html",
    "title": "nextflow.config",
    "section": "",
    "text": "nextflow.config\n\n\n\n\n\n\nRelevant Nextflow components\n\n\n\n\nUse the manifest scope to define metadata for publishing the pipeline.\nUse the nextflowVersion setting to set the minimum version of Nextflow.\nUse the params scope to define parameters accessible to the pipeline.\nUse the shell directive to define a custom shell command.\nUse the dag config scope to generate a workflow diagram.\nUse the report config scope to generate an html summary report.\nUse the timeline config scope to generate an html runtime report.\nUse the trace config scope to generate a task execution trace file.\nUse the profile scope to import profile configs from config/ directory.\nUse the process selector to specify default directives for processes.\n\n\n\nThis is the configuration script that Nextflow looks for when you run a workflow. It contains a number of property definitions that are used by the pipeline. A key feature of Nextflow is the ability to separate workflow implementation from the underlying execution platform using this configuration file.\nSince we can add additional configuration files for different run environments (i.e. job schedulers, use of singularity vs bioconda) each configuration file can contain conflicting settings and parameters listed in this file can be overwritten in the run command by specifying relevant commands.\nSee here for details on the hierarchy of configuration files.\n\nWhat’s in nextflow.config?\nThis file contains:\n\nA manifest for defining workflow metadata\nMandated minimal version of Nextflow required to run pipeline\nDefault workflow parameter definitions\nShell behaviour settings for the workflow\nExecution reports\nConfiguration profiles\nDefault resource definitions for processes\n\n\nManifest\nThe manifest scope allows you to define metadata required when publishing or running your pipeline. In this template, we’ve applied the following information to the workflow:\nmanifest {\n    author = 'Georgie Samaha'\n    name = 'Nextflow_DSL2_template-nf'\n    description = 'Template for creating Nextflow workflows'\n    homePage = 'https://github.com/Sydney-Informatics-Hub/Nextflow_DSL2_template'\n}\n\n\nNextflow version\nThe nextflowVersion setting allows you to mandate a minimum version of Nextflow that can be used to run your pipeline. In this template we have specified the minimal version of Nextflow that can handle DSL2 syntax:\nnextflowVersion = '!&gt;=20.07.1'\n\n\nParameters\nThe params scope allows you to define all parameters accessible in the pipeline script. Here, you can define default values to parameters which can then be overwritten using parameter flags (--) when the workflow is run. These defined parameters should correspond to parameters called by the process modules. For example:\nIn nextflow.config we define default parameters:\nparams.foo = 'Hello'\nparams.bar = 'world!'\nIn main.nf we define the workflow and specify our parameters as inputs to the process:\n// Include the process file \ninclude { PROCESS } from './modules/process.nf'\n\n// Run the workflow, calling the process called PROCESS \nworkflow {\n    PROCESS(params.foo, params.bar).view()\n}\nIn modules/process.nf we provide those parameters as input channels to the process and defined the output channel as standard output:\nprocess PROCESS {\ninput:\n    val(params.foo)\n    val(params.bar)\n\noutput:\n    stdout\n\nscript:\n    \"\"\"\n    echo \"$params.foo $params.bar\"\n    \"\"\"\n}\nRunning these scripts as:\nnextflow run main.nf\nGives the following output:\nN E X T F L O W  ~  version 22.10.3\nLaunching `main.nf` [angry_montalcini] DSL2 - revision: 33ed2edb33\nexecutor &gt;  local (1)\n[eb/25f43a] process &gt; PROCESS [100%] 1 of 1 ✔\nHello world!\nYou can override these parameters when you run the workflow, for example:\nnextflow run main.nf --foo goodbye \nGives the following output:\nN E X T F L O W  ~  version 22.10.3\nLaunching `main.nf` [maniac_hopper] DSL2 - revision: c7986b5056\nexecutor &gt;  local (1)\n[5c/33c2f0] process &gt; PROCESS [100%] 1 of 1 ✔\ngoodbye world!\n\n\nShell behaviour settings\nBy default for the most recent versions of Nextflow, workflows are executed with set -ue. Set is a Bash command used to control the different attributes and parameters of the shell environment. The -e flag ensures the workflow exits for non-zero exit status values and the -u flag traces unset variables. We have provided additional recommended shell settings to enhance error handling and pipeline reliability:\n\n'/bin/bash' tells Nextflow to use Bash as the shell to run process scripts\n'-euo', 'pipefail' tells Bash to consider a pipeline as failed if any command within it fails and to exit on error.\n\nshell = ['/bin/bash', '-euo', 'pipefail']\nYou can specify additional bash options to specify script execution and error management strategies.\n\n\nExecution reports\nNextflow provides a number of features for tracing and reporting on process execution status. In this template, we’ve asked Nextflow to generate info reports with dag{}, report{}, timeline{}, and trace{} scopes.\nDAG visualisation with dag{}\nA Nextflow pipeline’s direct acyclic graph (DAG) can be rendered by specifying the dag{} scope in the nextflow.config. In this template, we have:\n\nEnabled DAG creation upon workflow completion by default with enable = true\nEnabled the overwriting of existing DAGs with overwrite = true\nSpecified a file name and location to save the DAG with file = 'runInfo/dag.svg'\n\nTo generate the DAG, you must have GraphViz installed on your system.\nExecution report with report{}\nNextflow can create an HTML execution report that contains useful metrics about your workflow execution. See the Nextflow documentation for more information on its contents. Similarly, we have employed the same operators as above to output the execution report, however unlike a DAG you may wish to not overwrite existing execution reports, you can do this by specifying overwrite = false and naming your execution report based on timestamp, as was demonstrated here for a trace report:\n// Define timestamp, to avoid overwriting existing report \ndef timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\nreport {\n    enabled = true\n    overwrite = false\n    file = 'runInfo/report-${timestamp}.html'\n}\nTimeline summary with timeline{}\nNextflow can create an HTML timeline report for all tasks executed by your workflow. See the Nextflow documentation for more information on its contents. Same as with the dag and report features, you may wish to adjust the way these are output in the template.\nProcess resource trace with trace{}\nThis template also creates a simple resource trace report. A trace report can be customised to include any combination of available fields as demonstrated here. Consider customising the trace file generated by the workflow by adding the fields = option to specify any of the trace fields provided by Nextflow.\n\n\nConfiguration profiles\nWe have provided configuration files for specific infrastructures inside the config/ directory in this template. These are defined inside the nextflow.config file using the profiles{} scope. To keep the workflow modular and flexible, we’ve assigned profile names to these config files using includeConfig allowing these profile definitions to be activated when launching your pipeline with nextflow run main.nf -profile standard for example. These configuration files can be used to override defaults specified inside the nextflow.config. Add your own custom configuration to config/ directory and update the profiles scope within the nextflow.config with the following:\nprofiles {\n    standard    { includeConfig \"config/standard.config\"}\n    configName  { includeConfig \"config/my.config\"      }\n}\nSee here for more details on what sorts of things can be included here.\n\n\nDefault resource definitions\nThe process{} configuration scope can be used to provide default configurations for the processes in your workflow. In the template, we have set some default CPU and memory directives to be applied to all processes run by the workflow. There are many process directives you can apply here to control resource availability, software execution, interact with job schedulers etc.\nYou can take a more fine-grained approach to process configuration by using Nextflow’s withName process selector, as demonstrated here. For example, for a process called PROCESS, we could change the default settings applied at the workflow level with:\nprocess {\n    cpus = 1\n    memory = 5.Gb\n\n    withName: 'PROCESS' {\n        cpus = 2\n        memory = 10.Gb\n    }\n}",
    "crumbs": [
      "Home",
      "Template Components",
      "`nextflow.config`"
    ]
  },
  {
    "objectID": "notebooks/resources.html",
    "href": "notebooks/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Nextflow user guide\nNextflow quick start guide\nNextflow YouTube channel\n\n\n\n\n\nNextflow training workshop materials\nCustomising nf-core workshop materials\nIntro to Nextflow workflows\nNF-camp tutorial converting rnaseq-nf pipeline to DSL2\n\nDSL2 modules tutorial video\n\nIntro to DSL2 video\n\nNextflow for data intensive pipelines from Pawsey Supercomputing Center\nA self-guided DSL2 tutorial\n\n\n\n\n\nNextflow’s nf-core pipelines\nDSL2 pipeline structure walkthrough video from nf-core\n\n\n\n\n\nAustralian BioCommons workflow documentation guidelines"
  },
  {
    "objectID": "notebooks/resources.html#nextflow-documentation",
    "href": "notebooks/resources.html#nextflow-documentation",
    "title": "Resources",
    "section": "",
    "text": "Nextflow user guide\nNextflow quick start guide\nNextflow YouTube channel"
  },
  {
    "objectID": "notebooks/resources.html#training",
    "href": "notebooks/resources.html#training",
    "title": "Resources",
    "section": "",
    "text": "Nextflow training workshop materials\nCustomising nf-core workshop materials\nIntro to Nextflow workflows\nNF-camp tutorial converting rnaseq-nf pipeline to DSL2\n\nDSL2 modules tutorial video\n\nIntro to DSL2 video\n\nNextflow for data intensive pipelines from Pawsey Supercomputing Center\nA self-guided DSL2 tutorial"
  },
  {
    "objectID": "notebooks/resources.html#nf-core",
    "href": "notebooks/resources.html#nf-core",
    "title": "Resources",
    "section": "",
    "text": "Nextflow’s nf-core pipelines\nDSL2 pipeline structure walkthrough video from nf-core"
  },
  {
    "objectID": "notebooks/resources.html#australian-biocommons",
    "href": "notebooks/resources.html#australian-biocommons",
    "title": "Resources",
    "section": "",
    "text": "Australian BioCommons workflow documentation guidelines"
  },
  {
    "objectID": "notebooks/channels.html",
    "href": "notebooks/channels.html",
    "title": "Building channels",
    "section": "",
    "text": "In Nextflow, channels are require to move data between processes. Channels act as communication links that pass data between tasks in a workflow. Using channels effectively ensures smooth and efficient execution. Let’s explore some best practices and common scenarios for building channels, and how Groovy and Nextflow operators can be used to manipulate data within them.",
    "crumbs": [
      "Home",
      "Best Practices",
      "**Building channels**"
    ]
  },
  {
    "objectID": "notebooks/channels.html#channel-creation-and-initialization",
    "href": "notebooks/channels.html#channel-creation-and-initialization",
    "title": "Building channels",
    "section": "1. Channel Creation and Initialization",
    "text": "1. Channel Creation and Initialization",
    "crumbs": [
      "Home",
      "Best Practices",
      "**Building channels**"
    ]
  },
  {
    "objectID": "notebooks/channels.html#channel-creation-and-initialisation",
    "href": "notebooks/channels.html#channel-creation-and-initialisation",
    "title": "Building channels",
    "section": "Channel Creation and Initialisation",
    "text": "Channel Creation and Initialisation\nChannels can be created in different ways, such as from lists, files, or even functions. For instance, a simple channel from a file can be defined as:\n// Define a channel from a file\nChannel.fromPath('samplesheet.csv')",
    "crumbs": [
      "Home",
      "Best Practices",
      "**Building channels**"
    ]
  },
  {
    "objectID": "notebooks/channels.html#some-common-scenarios",
    "href": "notebooks/channels.html#some-common-scenarios",
    "title": "Building channels",
    "section": "Some common scenarios",
    "text": "Some common scenarios\n\nHandling paired-end sequencing data\n\nUse case: generating a report with optional outputs\n\n\n\nCreating channels from samplesheets\n\nUse case: generating a report with optional outputs\n\n\n\nDynamic input selection\n\nUse case: generating a report with optional outputs\n\n\n\nGroup data by a metadata field\nIn bioinformatics workflows, it’s common to process data based on specific attributes, such as grouping samples by tissue type, batch, or condition. This can be useful when performing differential expression analysis or organising data for downstream steps.\n\nUse case: grouping data for batch analysis\nConsider a scenario where you have RNAseq data from multiple tissues (e.g. liver, kidney, brain) listed in a CSV file. To perform differential analysis per tissue type, you can group samples by the tissue_type field and analyse each group separately.\nKey Steps:\n\nExtract sample information from the metadata (e.g., tissue type) using a CSV.\nUse the groupBy operator to organise samples by the tissue type field.\nRun processes on each group independently, enabling parallel analysis.\n\nsamplesheet = Channel.fromPath('samples.csv').splitCsv(header: true)\n\n// Group samples by tissue type\ngrouped_samples = samplesheet.groupTuple { it.tissue_type }\n\nprocess analyze_by_group {\n    input:\n    val tissue_type\n    val samples from grouped_samples\n\n    output:\n    path \"results/${tissue_type}.tsv\"\n\n    script:\n    \"\"\"\n    run_analysis --input ${samples} --output results/${tissue_type}.tsv\n    \"\"\"\n}\n\n\n\nSkipping empty or optional data\nThe ifEmpty operator allows us to define a fallback channel if the original channel is empty. This is helpful when working with optional outputs (as specified in the output: block, using optional: true).\n\nUse case: conditionally outputting results from a process\nIn workflows, you may need to handle cases where processes generate optional outputs. In this case, Plassembler outputs plasmid detection files, but not all samples will contain plasmids. To manage this, you can combine optional outputs with ifEmpty to control what gets passed downstream.\nKey steps:\n\nRun plassembler to detect plasmids.\nUse ifEmpty to handle cases where plasmids .fasta is empty or missing.\nUse optional: true for downstream processes (e.g. Bakta) that depend on plasmid detection\n\nprocess plassembler {\n  tag \"DETECTING PLASMIDS AND OTHER MOBILE ELEMENTS: ${barcode}\"\n\n  input:\n  tuple val(barcode), path(trimmed_fq), path(flye_assembly)\n  path plassembler_db \n\n  output:\n  tuple val(barcode), path(\"plassembler/plasmids.fasta\"), optional: true\n  tuple val(barcode), path(\"plassembler/summary.tsv\")\n\n  script:\n  \"\"\"\n  plassembler long \\\\\n    -d ${plassembler_db} \\\\\n    -l ${trimmed_fq} \\\\\n    --flye_assembly ${flye_assembly} \\\\\n    -o plassembler \\\\\n    -t ${task.cpus}\n  \"\"\"\n}\nNow, we can define the bakta_plasmids input channel as:\nbakta_plasmid = plassembler.out.plassembler_fasta.ifEmpty([])\n\nbakta_plasmid channel will receive the output from the Plassembler process.\nifEmpty([]) ensures that if no plasmid .fasta file is generated, an empty list [] is passed instead.\n\nThis channel can now be used as input to downstream processes, like Bakta, with conditional execution based on whether plasmids were detected.",
    "crumbs": [
      "Home",
      "Best Practices",
      "**Building channels**"
    ]
  },
  {
    "objectID": "notebooks/channels.html#some-tricky-scenarios",
    "href": "notebooks/channels.html#some-tricky-scenarios",
    "title": "Building channels",
    "section": "Some tricky scenarios",
    "text": "Some tricky scenarios\n\nCreating channels from samplesheets\nIn bioinformatics workflows, samplesheets (as CSV files) are a common way to store metadata about sequencing experiments, such as sample names, file paths, and experimental conditions. Nextflow allows you to easily create channels from these samplesheets, enabling efficient parallel processing of the associated data.\n\nUse case: grouping fastq pairs according to their sample ID\nConsider a samplesheet with the following structure:\n```default\nsample,tissue_type,fq1,fq2\nsample1,liver,sample1_liver_1.fq,sample1_liver_2.fq\nsample2,kidney,sample2_kidney_1.fq,sample2_kidney_2.fq\nsample3,brain,sample3_brain_1.fq,sample3_brain_2.fq\nsample4,liver,sample4_liver_1.fq,sample4_liver_2.fq\nsample5,kidney,sample5_kidney_1.fq,sample5_kidney_2.fq\nWe need to input fq pairs according to their sample ID to bwa-mem2 for alignment to a reference assembly. We can do this with the channel:\nsamplesheet = Channel.fromPath('samples.csv')\n              .splitCsv(header: true)\n              .map { row -&gt; tuple(row.sample, row.tissue_type, row.fq1, row.fq2) }\n\nsplitCsv(header: true) parses the CSV with headers.\nmap transforms each row into a tuple containing relevant fields\n\nprocess align_reads {\n    \n  input:\n  tuple val(sample), path(fq1), path(fq2)\n  path reference_index\n\n  output:\n  path \"${sample}.bam\"\n\n  script:\n  \"\"\"\n  bwa-mem2 mem ${reference_index} ${fq1} ${fq2} \\\\\n    -t ${task.cpus} \\\\\n    | samtools view -Sb - \\\\\n    &gt; ${sample}.bam\n  \"\"\"\n}\nThis allows you to process samples in parallel while maintaining consistent metadata and file associations for each sample across all workflow steps.\n\n\n\nDynamic input selection\nIn Nextflow allows workflows to intelligently handle variable inputs, making them adaptable to different scenarios. This is especially useful when your input data or conditions change across runs, for example checking if a resource like a database or index already exists. If it does, the workflow uses it directly. If not, it dynamically runs a process to generate or download the resource.\n\nUse case: dynamically selecting to run a process\nConsider a scenario where we need to check if the Kraken2 database exists and either use the existing file or download/build it if it’s missing.\nif (params.kraken_db) {\n    kraken2_db = path(params.kraken_db)\n\n} else {\n    process download_k2db {\n      output:\n      path \"kraken2_db\", emit: k2db\n\n      script:\n      \"\"\"\n      kraken2-build --download-library bacteria --db kraken2_db\n      \"\"\"\n   }\n\n    kraken2_db = download_kraken2_db.out.k2db\n}\n\nif (params.kraken_db) checks if the kraken_db parameter (--kraken_db) is provided in the run command. If it is, it defines the kraken2_db channel.\nelse {} block: if --kraken_db is not provided, it runs the download_k2db process and assigns the output to the kraken2_db channel.\n\nThis structure makes the workflow flexible by either using an existing resource or generating it if necessary.\n\n\n\nGroup data by a metadata field\nIn bioinformatics workflows, it’s common to process data based on specific attributes, such as grouping samples by tissue type, batch, or condition. This can be useful when performing differential expression analysis or organising data for downstream steps.\n\nUse case: grouping data for batch analysis\nConsider a scenario where you have RNAseq data from multiple tissues (e.g. liver, kidney, brain) listed in a CSV file. To perform differential analysis per tissue type, you can group samples by the tissue_type field and analyse each group separately.\nKey Steps:\n\nExtract sample information from the metadata (e.g., tissue type) using a CSV.\nUse the groupBy operator to organise samples by the tissue type field.\nRun processes on each group independently, enabling parallel analysis.\n\nConsider the theoretical samplesheet containing the fields:\n\nsample: The unique identifier for each sample.\ntissue_type: The tissue from which the sample was taken.\nfq1, fq2: Paths to paired-end fastq files for RNAseq data.\n\nsample,tissue_type,fq1,fq2\nsample1,liver,sample1_liver_1.fq,sample1_liver_2.fq\nsample2,kidney,sample2_kidney_1.fq,sample2_kidney_2.fq\nsample3,brain,sample3_brain_1.fq,sample3_brain_2.fq\nsample4,liver,sample4_liver_1.fq,sample4_liver_2.fq\nsample5,kidney,sample5_kidney_1.fq,sample5_kidney_2.fq\nThe channel for this samplesheet structure:\nsamplesheet = Channel.fromPath('samples.csv')\n              .splitCsv(header: true)\n              .map { row -&gt; tuple(row.tissue_type, \n                            tuple(row.sample, row.fq1, row.fq2)) }\n              .groupTuple()\n\nsplitCsv(header: true) reads the CSV file with headers.\nmap extracts the sample, tissue type, fq1, and fq2 file paths.\ngroupTuple(by: 1) groups the samples by tissue type (index 1 refers to the second column, which is tissue_type).\n\nThis will create groups based on tissue types for downstream analysis, enabling you to process samples by their respective tissues.\nprocess salmon_quant {\n\n  input:\n  tuple val(tissue_type), tuple val(sample), path(fq1), path(fq2)\n  path(transcriptome_index)\n\n  output:\n  path \"results/${tissue_type}/${sample}_quant.sf\"\n\n  script:\n  \"\"\"\n  salmon quant \\\\\n    -i transcriptome_index \\\\\n    -l A \\\\\n    -1 ${fq1} -2 ${fq2} \\\\\n    -o ${tissue_type}/${sample}\n  \"\"\"\n}\nThis ensures correct association between tissue, sample, and paired-end files for RNAseq quantification.\n\n\n\nSkipping empty or optional data\nThe ifEmpty operator allows us to define a fallback channel if the original channel is empty. This is helpful when working with optional outputs (as specified in the output: block, using optional: true).\n\nUse case: conditionally outputting results from a process\nIn workflows, you may need to handle cases where processes generate optional outputs. In this case, Plassembler outputs plasmid detection files, but not all samples will contain plasmids. To manage this, you can combine optional outputs with ifEmpty to control what gets passed downstream.\nKey steps:\n\nRun plassembler to detect plasmids.\nUse ifEmpty to handle cases where plasmids .fasta is empty or missing.\nUse optional: true for downstream processes (e.g. Bakta) that depend on plasmid detection\n\nprocess plassembler {\n  tag \"DETECTING PLASMIDS AND OTHER MOBILE ELEMENTS: ${barcode}\"\n\n  input:\n  tuple val(barcode), path(trimmed_fq), path(flye_assembly)\n  path plassembler_db \n\n  output:\n  tuple val(barcode), path(\"plassembler/plasmids.fasta\"), optional: true\n  tuple val(barcode), path(\"plassembler/summary.tsv\")\n\n  script:\n  \"\"\"\n  plassembler long \\\\\n    -d ${plassembler_db} \\\\\n    -l ${trimmed_fq} \\\\\n    --flye_assembly ${flye_assembly} \\\\\n    -o plassembler \\\\\n    -t ${task.cpus}\n  \"\"\"\n}\nNow, we can define the bakta_plasmids input channel as:\nbakta_plasmid = plassembler.out.plassembler_fasta.ifEmpty([])\n\nbakta_plasmid channel will receive the output from the Plassembler process.\nifEmpty([]) ensures that if no plasmid .fasta file is generated, an empty list [] is passed instead.\n\nThis channel can now be used as input to downstream processes, like Bakta, with conditional execution based on whether plasmids were detected.",
    "crumbs": [
      "Home",
      "Best Practices",
      "**Building channels**"
    ]
  }
]